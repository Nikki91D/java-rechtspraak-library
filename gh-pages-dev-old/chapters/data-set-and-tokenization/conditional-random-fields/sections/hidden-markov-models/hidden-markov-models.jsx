var React = require('react');
var PureRenderMixin = require('react-addons-pure-render-mixin');

var F = require('../../../../math');
var FigRef = require('../../../../figures/fig-ref');
var FigImg = require('../../../../figures/fig-image');
var figs = require('../../../../figures/figs');
var hmm = require('../../sections').hmm;

var HmmComponent = React.createClass({
    mixins: [PureRenderMixin],
    getDefaultProps: function () {
        return {}
    },


    render: function () {

        return <section id={hmm.id}>
            <h2>{hmm.title}</h2>

            <p>
                Hidden Markov Models (HMMs) are a subclass of graphical models in which
                we model a linear sequence of observations <F l="\mathbf x=\{x_t\}_{t=1}^T"/> that are assumed
                to be the result of a sequence of hidden states <F l="\mathbf y=\{y_t\}_{t=1}^T"/>.
                One example of an application would be spoken word recognition, in which
                samples of the sound waves can be seen as observations, and the actual phonemes as the hidden states.
            </p>

            <p>
                To assure computational tractability, HMMs make use of the Markov assumption,
                which is that:
            </p>

            <ol>
                <li>any output variable (or <em>label</em>) <F l="y_t"/> only depends
                    on <F l="y_{t-1}"/>, where the initial probability <F l="p(y_{1})"/> is given
                </li>
                <li>any input variable (or <em>observation</em>) <F l="x_t"/> only depends on the label <F l="y_t"/>; we
                    say that the observation <F l="x_t"/> is generated by label <F l="y_t"/>.
                </li>
            </ol>

            <p>A HMM is then a graphical model which factorizes as follows:

                <F display="true"
                   l="p\left ( \mathbf x, \mathbf y\right ) = \prod _{t=1}^T p(x_t)p(y_t) = \prod _{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})"/>
            </p>

            <p>
                If we return to the representation of HMMs in <FigRef fig={figs.graphicalModels}/>, we see then that the
                white nodes represent labels
                and the grey nodes
                the observations. Typically, observations are known, but the labels need to be estimated. This is done
                looping over different output values and selecting <F l="\mathbf y*"/> with the highest likelihood.
            </p>

            <p>
                To find plausible values of <F l="p(x_t|y_t)"/> and <F l="p(y_t|y_{t-1})"/>, we
                typically use a set of known
                observation-label sequences with a parameter estimation method such as the <a
                className="wiki"
                href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Baum-Welch algorithm</a>[TODO ref?].
            </p>
        </section>;
    }
});

module.exports = HmmComponent;