//noinspection JSUnresolvedVariable
import React, {Component} from 'react';
import FigRef from './../../../../Figures/FigRef'
import FigImg from './../../../../Figures/Image/Image'
import figs from './../../../../Figures/figs'
import ref from '../../../../Bibliography/References/references'
import bib from  '../../../../Bibliography/bib';
import F from  '../../../../Math/Math';
import abbrs from  '../../../../abbreviations';

export default class CRF extends Component {
    render() {
        return <div>
            <p>
                Directed Graphical Models (or Bayesian Networks)
                are statistical models that model some probability distribution
                over variables in a set <F l="V"/> which take values from a set <F l="\mathcal{V}"/>.
                Directed Graphical Models can be represented as a directed graph where
                nodes represent the variables in <F l="V"/>, and the edges represent conditional dependencies.
                Directed graphical models
                factorize as follows:
                <F l="p( \mathbf x, \mathbf y)=\prod _{v\in V}p(v|\pi(v))" display="true"/>
                where <F l="\pi(v)"/> are the parents of <F l="v"/> in <F l="G"/>. 
            </p>
            <p>
                Hidden Markov Models ({abbrs.hmms}) are one instance of directed models
                in which we have a linear sequence of
                observations <F l="\mathbf x=\{\mathbf x_t\}_{t=1}^T"/> that is assumed
                to be generated by a sequence of hidden states <F l="\mathbf y=\{\mathbf y_t\}_{t=1}^T"/>.
                One example of an application would be speech recognition, in which
                samples of the sound waves can be seen as observations, and the actual phonemes as the hidden
                states.
            </p>


            <p>
                To assure computational tractability, {abbrs.hmms} make use of the Markov assumption,
                which is that:
            </p>

            <ol>
                <li>any output variable (or <em>label</em>) <F l="\mathbf y_t"/> only depends
                    on <F l="\mathbf y_{t-1}"/>, where the initial probability <F l="p(\mathbf y_{1})"/> is given
                </li>
                <li>any input variable (or <em>observation</em>) <F l="\mathbf x_t"/> only depends on the label <F
                    l="\mathbf y_t"/>;
                    the observation <F l="\mathbf x_t"/> is generated by label <F l="\mathbf y_t"/>.
                </li>
            </ol>

            <p>A {abbrs.hmm} then factorizes as follows:

                <F display="true"
                   l="p\left ( \mathbf x, \mathbf y\right ) = \prod _{t=1}^T p(x_t)p(y_t) = \prod _{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})"/>
            </p>

            <p>
                If we return to the representation of {abbrs.hmms} in <FigRef fig={figs.graphicalModels}/>, we see that
                the
                white nodes represent labels
                and the grey nodes
                the observations. Typically, observations are given, but the labels need to be guessed. This is
                done by
                looping over all <F l="\mathbf y\in Y"/> and selecting <F l="\mathbf y^*\in Y"/> with the
                highest likelihood.
            </p>

            <p>
                To find plausible values of <F l="p(\mathbf x_t|\mathbf y_t)"/> and <F
                l="p(\mathbf y_t|\mathbf y_{t-1})"/>, we
                typically use a set of pre-tagged
                observation-label sequences and perform a parameter estimation method such as the <a
                className="wiki"
                href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Baum-Welch
                algorithm</a> ({ref.cite(bib.lucke1996stochastic)}).
            </p>
        </div>;
    }
}
            