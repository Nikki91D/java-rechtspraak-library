//noinspection JSUnresolvedVariable
import React, {Component} from 'react';
import FigRef from './../../../../Figures/FigRef'
import FigImg from './../../../../Figures/Image/Image'
import figs from './../../../../Figures/figs'
import ref from '../../../../Bibliography/References/references'
import bib from  '../../../../Bibliography/bib';
import F from  '../../../../Math/Math';
import abbrs from  '../../../../abbreviations';

export default class CRF extends Component {
    render() {
        return <div>
            <p>
                Directed Graphical Models (or Bayesian Networks)
                are statistical models that model some probability distribution
                over variables <F l="v"/> in a set <F l="V"/> which take values from a set <F l="\mathcal{V}"/>.
                Loosely speaking, Directed Graphical Models can be represented as a directed graph where
                nodes represent the variables <F l="v \in V"/>, and the edges represent dependencies.
                Directed graphical models
                factorize as follows:
                <F l="p(v)=\prod _{v\in V}p(v|\pi(v))" display="true"/>
                where <F l="\pi(v)"/> are the parents of node <F l="v"/> in graph <F l="G"/>.
            </p>
            <p>
                Hidden Markov Models ({abbrs.hmms}) are one instance of directed models
                which have a linear sequences of
                observations <F l="\mathbf x=\{x_t\}_{t=1}^T"/> and a linear sequence of
                labels (in HMM parlance, 'hidden
                states') <F l="\mathbf y=\{y_t\}_{t=1}^T"/>, which are valuations
                of some random vectors <F l="X"/> and <F l="Y"/> respectively, and <F 
                l="V = X\cup Y"/>.
                In HMMs, the observations <F l="\mathbf x=\{x_t\}_{t=1}^T"/> are assumed
                to be generated by the labels.
                One example of an application would be speech recognition, in which
                samples of the sound waves can be seen as observations, and the actual phonemes as the labels.
            </p>

            <p>
                To assure computational tractability, {abbrs.hmms} make use of the Markov assumption,
                which is that:
            </p>

            <ol>
                <li>any label <F l="y_t"/> only depends
                    on <F l="y_{t-1}"/>, where the initial probability <F l="p(y_{1})"/> is given
                </li>
                <li>any observation <F l="x_t"/> only depends on the label <F
                    l="y_t"/>;
                    the observation <F l="x_t"/> is generated by label <F l="y_t"/>.
                </li>
            </ol>

            <p>A {abbrs.hmm} then factorizes as follows:

                <F display="true"
                   l="p\left (\mathbf x,\mathbf y \right )= \prod _{t=1}^T p(x_t)p(y_t) = \prod _{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})"/>
            </p>

            <p>
                If we return to the representation of {abbrs.hmms} in <FigRef fig={figs.graphicalModels}/>, we see that
                the
                white nodes represent labels
                and the grey nodes
                the observations. Typically, observations are given, but the labels need to be inferred. This is
                done from a given HMM model by
                looping over all <F l="\mathbf y\in Y"/> and selecting <F l="\mathbf y^*\in Y"/> with the
                highest likelihood.
            </p>

            <p>
                To find a model with plausible values of <F l="p(x_t|y_t)"/> and <F
                l="p(y_t|y_{t-1})"/>, we
                typically use a set of pre-tagged
                observation-label sequences and perform a parameter estimation method such as the <a
                className="wiki"
                href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Baum-Welch
                algorithm</a> ({ref.cite(bib.lucke1996stochastic)}).
            </p>
        </div>;
    }
}
            