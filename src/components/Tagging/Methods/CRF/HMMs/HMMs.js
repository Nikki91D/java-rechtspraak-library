//noinspection JSUnresolvedVariable
import React, {Component} from 'react';
import FigRef from './../../../../Figures/FigRef'
import FigImg from './../../../../Figures/Image/Image'
import figs from './../../../../Figures/figs'
import ref from '../../../../Bibliography/References/references'
import bib from  '../../../../Bibliography/bib';
import F from  '../../../../Math/Math';

export default class CRF extends Component {
    render() {
        return <section id="hmm">
            <h4>Hidden Markov Models</h4>

            <p>
                Hidden Markov Models (HMMs) are a subclass of graphical models in which
                we model a linear sequence of observations <F l="\mathbf x=\{x_t\}_{t=1}^T"/> that are assumed
                to be generated by a sequence of hidden states <F l="\mathbf y=\{y_t\}_{t=1}^T"/>.
                One example of an application would be spoken word recognition, in which
                samples of the sound waves can be seen as observations, and the actual phonemes as the hidden
                states.
            </p>

            <p>
                To assure computational tractability, HMMs make use of the Markov assumption,
                which is that:
            </p>

            <ol>
                <li>any output variable (or <em>label</em>) <F l="y_t"/> only depends
                    on <F l="y_{t-1}"/>, where the initial probability <F l="p(y_{1})"/> is given
                </li>
                <li>any input variable (or <em>observation</em>) <F l="x_t"/> only depends on the label <F l="y_t"/>;
                    the observation <F l="x_t"/> is generated by label <F l="y_t"/>.
                </li>
            </ol>

            <p>A HMM is then a graphical model which factorizes as follows:

                <F display="true"
                   l="p\left ( \mathbf x, \mathbf y\right ) = \prod _{t=1}^T p(x_t)p(y_t) = \prod _{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})"/>
            </p>

            <p>
                If we return to the representation of HMMs in <FigRef fig={figs.graphicalModels}/>, we see then that
                the
                white nodes represent labels
                and the grey nodes
                the observations. Typically, observations are known, but the labels need to be estimated. This is
                done by
                looping over all <F l="\mathbf y\in Y"/> and selecting <F l="\mathbf y^*\in Y"/> with the highest likelihood.
            </p>

            <p>
                To find plausible values of <F l="p(x_t|y_t)"/> and <F l="p(y_t|y_{t-1})"/>, we
                typically use a set of known
                observation-label sequences with a parameter estimation method such as the <a
                className="wiki"
                href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Baum-Welch algorithm</a>[TODO
                ref?].
            </p>
        </section>;
    }
}
            